
# ğŸšš IPPO Multi-Agent Dynamic Vehicle Routing Problem (DVRP)

A **multi-agent reinforcement learning** framework for the **Dynamic Capacitated Vehicle Routing Problem with Time Windows (DVRP-TW)** using **Independent Proximal Policy Optimization (IPPO)**.

---

## âœ¨ Features

* ğŸ§© **Custom DVRP environment** with dynamic order arrivals, time windows, and shared penalties.
* ğŸ¤– **IPPO agents** with action masking, reward shaping, and per-agent logging.
* ğŸ‹ï¸ **Training script** with checkpointing and resume support.
* ğŸ¬ **Live visualizer** for real-time animated simulations.

---

## âš™ï¸ Installation

```bash
git clone https://github.com/yourname/ippo-dvrp.git
cd ippo-dvrp
```

---

## ğŸš€ Training

```bash
python train_ippo.py
```

---

## â–¶ï¸ Resume Training

```python
agents = run_ippo(
    env,
    load_path="checkpoint/iter_750",  # Resume from checkpoint
    num_iterations=2000
)
```

---

## ğŸ¥ Visualization (Live Animation)

```bash
python visualizer.py
```

### In Jupyter Notebook

```python
%matplotlib notebook
```

### In Python Script

`plt.ion()` is already enabled.

---

## ğŸ—ºï¸ What Youâ€™ll See

| Symbol             | Meaning                               |
| ------------------ | ------------------------------------- |
| ğŸŸ¥ Red Square      | Depot (restock point)                 |
| ğŸ”µ Colored Circles | Agents (labeled 0, 1, 2)              |
| ğŸŸ¡ Gold Circles    | Available orders                      |
| â­ Gold Stars       | Accepted orders                       |
| ğŸ§¾ Info Panel      | Time, success rate, rewards, distance |

Agents move smoothly step-by-step across the grid.

---

## ğŸ“Š Evaluation Metrics

At the end of each episode:

```
Success Rate           = delivered / total_orders
Avg Distance per Agent = total_dist / M
Objective (lower better) = 0.5 Ã— (10 Ã— (1 - success) + avg_dist / 480)
```

---

## ğŸ§  Tips for Better Performance

* Increase `rollout_steps` to **480+** for longer episodes.
* Tune `Î±` (0.2â€“0.5) to balance exploration vs. delivery focus.
* Enable normalization (`use_normalization=True`) if observations vary widely.
* Use **GPU** for faster training.

---

## ğŸ‘¥ Contributors

* **Nguyá»…n Quang Huy** â€“ Core Developer

---

## ğŸ“„ License

Feel free to **use, modify, and distribute** this project under an open license.

---
